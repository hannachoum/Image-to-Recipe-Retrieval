{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/hanna.mergui/Computer-Vision/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device,\"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_bert_path = \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Summaries/export_summary_Bert.csv\"\n",
    "summary_bert = pd.read_csv(summary_bert_path)\n",
    "\n",
    "\n",
    "summary_bert_ingredients_path=\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Summaries/export_summary_bert_with_ingredients.csv\"\n",
    "summary_bert_ingredients=pd.read_csv(summary_bert_ingredients_path)\n",
    "\n",
    "data_dir = \"ComputerVision_Data\"\n",
    "images_dir = \"Food Images/Food Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sumary recipe  13463\n",
      "number of images  13582\n"
     ]
    }
   ],
   "source": [
    "recipe = summary_bert[\"Summary\"]\n",
    "print(\"number of sumary recipe \",len(recipe))\n",
    "\n",
    "images = os.listdir(os.path.join(data_dir, images_dir))\n",
    "print(\"number of images \",len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'ViT-B/32']\n"
     ]
    }
   ],
   "source": [
    "models = clip.available_models()\n",
    "print(models)\n",
    "model, preprocess = clip.load('RN50', device,jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With bert Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13463\n",
      "13406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Tensor creation and array creation\n",
    "\n",
    "import string\n",
    "\n",
    "context_length=77\n",
    "array_text=[]\n",
    "count=0\n",
    "\n",
    "\n",
    "# Creation of a tensor for the text \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(device,\"device\")\n",
    "text_tensor = torch.zeros(len(summary_bert), context_length, dtype=torch.long)\n",
    "labels = []\n",
    "for i, row in enumerate(summary_bert.iterrows()):\n",
    "    summary = row[1][\"Summary\"]\n",
    "\n",
    "    to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "    summary = summary.translate(str.maketrans(\"\", \"\",to_supress)) # Remove punctuation from summary\n",
    "  \n",
    "    #Because now, we have the context_length probem\n",
    "    if len(summary)>context_length:\n",
    "        count+=1\n",
    "        summary = summary[:context_length]\n",
    "    \n",
    "            \n",
    "    array_text.append(summary)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "#torch.save(array_text, \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/tensor_summary_bert.pt\")\n",
    "    \n",
    "print(len(array_text))\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13463, 77])\n",
      "13463\n"
     ]
    }
   ],
   "source": [
    "text_tensor = torch.load(\"tensor_summary_bert.pt\")\n",
    "print(text_tensor.shape)\n",
    "print(len(array_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_inputs=[]\n",
    "\n",
    "for im in images:\n",
    "    im_path = os.path.join(data_dir, images_dir, im)\n",
    "    images_inputs.append(preprocess(Image.open(im_path)))\n",
    "images_inputs_tensor = torch.tensor(np.stack(images_inputs)).to(device)\n",
    "torch.save(images_inputs_tensor, \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/images_inputs_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_tensor shape torch.Size([13463, 77])\n",
      "images array shape 3486\n"
     ]
    }
   ],
   "source": [
    "print(\"text_tensor shape\", text_tensor.shape)\n",
    "#print(\"images_inputs_tensor shape \", images_inputs_tensor.shape)\n",
    "\n",
    "print(\"images array shape\",len(images_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "13463\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(array_text))\u001b[38;5;66;03m#list\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(array_text)) \u001b[38;5;66;03m#13463\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m,image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m#torch.Size([1, 3, 224, 224])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,text\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m#torch.Size([13463, 150])\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "image = preprocess(Image.open(\"ComputerVision_Data/Food Images/Food Images/-bloody-mary-tomato-toast-with-celery-and-horseradish-56389813.jpg\")).unsqueeze(0).to(device)\n",
    "#text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "\n",
    "print(type(array_text))#list\n",
    "print(len(array_text)) #13463\n",
    "\n",
    "\n",
    "text = clip.tokenize(array_text).to(device)\n",
    "\n",
    "print(\"image\",image.shape) #torch.Size([1, 3, 224, 224])\n",
    "print(\"text\",text.shape) #torch.Size([13463, 150])\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "    print(\"type image_features\",type(image_features.shape)) #<class 'torch.Size'>\n",
    "    print(\"shape image_features\",image_features.shape) #torch.Size([1, 1024]\n",
    "    \n",
    "    text_features = model.encode_text(text)\n",
    "    print(\"type text_features\",type(text_features.shape)) #<class 'torch.Size'>\n",
    "    print(\"shape text_features\",text_features.shape)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "#Note : normal qu'il y a un out of memory, je donne une liste et pas un tensor. Il faut changer ça. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ONLY the main ingredients (GPT3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minutesWhole chicken', 'whites potatoes', 'cream cheese', 'turkeyBread cubes', 'sugar bourbon']\n",
      "['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']\n"
     ]
    }
   ],
   "source": [
    "# Creation of a tensor for the text \n",
    "\n",
    "liste_ingredients=[]\n",
    "\n",
    "context_length=77\n",
    "#print(device,\"device\")\n",
    "liste_ingredients_tensor = torch.zeros(len(summary_bert_ingredients), context_length, dtype=torch.long)\n",
    "labels = []\n",
    "\n",
    "\n",
    "\n",
    "for i, row in enumerate(summary_bert_ingredients.iterrows()):\n",
    "    \n",
    "    summary = row[1][\"summary_with_ingredients\"]\n",
    "    ingredients = summary.split()[-2:]\n",
    "    \n",
    "    ingredients=str(ingredients[0])+ \" \" +str(ingredients[1])\n",
    "\n",
    "    to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "    ingredients = ingredients.translate(str.maketrans(\"\", \"\",to_supress)) # Remove punctuation from summary\n",
    "\n",
    "    liste_ingredients.append(ingredients)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "torch.save(liste_ingredients_tensor, \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/Main_ingredients_tensor.pt\")\n",
    "    \n",
    "print(liste_ingredients[:5])\n",
    "print(labels[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST with the 5 first line\n",
    "\n",
    "main_ingr= ['minutesWhole chicken', 'whites potatoes', 'cream cheese', 'turkeyBread cubes', 'sugar bourbon']\n",
    "label = ['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([0.1512, 0.2739, 0.0020, 0.5620, 0.0108], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Value: 0.562 for index : 3\n",
      "Value: 0.274 for index : 1\n",
      "Value: 0.1512 for index : 0\n",
      "Value: 0.01079 for index : 4\n",
      "Value: 0.001995 for index : 2\n",
      "3\n",
      "turkeyBread cubes\n",
      "Bread can be toasted 3 days ahead and kept once cool in a sealed bag at room \n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(main_ingr).to(device)\n",
    "print(type(text))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "    \n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(liste_ingredients[max_index])\n",
    "print(array_text[max_index])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With all the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([1.4353e-04, 2.4414e-04, 3.1531e-05,  ..., 6.0737e-05, 2.0325e-05,\n",
      "        1.0118e-03], device='cuda:0', dtype=torch.float16)\n",
      "Value: 0.10645 for index : 58\n",
      "Value: 0.0755 for index : 919\n",
      "Value: 0.03403 for index : 1004\n",
      "Value: 0.03099 for index : 906\n",
      "Value: 0.0305 for index : 116\n",
      "58\n",
      "yolks kefir\n",
      "Melt butter over medium heat in a large skillet Add pine nuts and cook until \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo verify the image \\n\\nfor i in range(len(labels)): \\n    if i==index: \\n        image_to_print.append(labels[i])\\n\\n\\n# Display the images using matplotlib\\nfig, axes = plt.subplots(1, len(image_to_print), figsize=(15, 3), squeeze=False)\\n\\nfor i, image_name in enumerate(image_to_print):\\n    image_path = os.path.join(images_dir, image_name)\\n    image = Image.open(image_path)\\n    axes[0, i].imshow(image)\\n    axes[0, i].set_title(image_name)\\n    axes[0, i].axis('off')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_to_print=[]\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/caramelized-plantain-parfait.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "text = clip.tokenize(liste_ingredients).to(device)\n",
    "print(type(text))\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(liste_ingredients[max_index])\n",
    "print(array_text[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With all ingredients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whole chicken kosher salt divided plus moresmall acorn squash about total fin', ' large egg whitespound new potatoes aboutinch in diameterteaspoons kosher sal', ' cup evaporated milkcup whole milk garlic powder onion powder smoked paprika ', 'to pound round Italian loaf cut into inch cubescupstablespoons olive oil divi', ' teaspoon dark brown sugarteaspoon hot water oz bourbonoz fresh lemon juicete']\n",
      "['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']\n"
     ]
    }
   ],
   "source": [
    "ingr_text=[]\n",
    "labels = []\n",
    "context_length=77\n",
    "\n",
    "liste_ingredients_tensor = torch.zeros(len(summary_bert), context_length, dtype=torch.long)\n",
    "\n",
    "\n",
    "for i, row in enumerate(summary_bert.iterrows()):\n",
    "    ingredients = row[1][\"Cleaned_Ingredients\"]\n",
    "\n",
    "    to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "    ingredients = ingredients.translate(str.maketrans(\"\", \"\",to_supress)) # Remove punctuation from summary\n",
    "    ingredients = ''.join([char for char in ingredients if not char.isdigit()])\n",
    "    ingredients = ingredients.replace(\"Tbsp\", \"\").replace(\"½\", \"\").replace(\"¾\", \"\").replace(\"lb\", \"\").replace(\"tsp\", \"\").replace(\"⅓\", \"\").replace(\"  \", \"\").replace(\"¼\", \"\")\n",
    "\n",
    "    #bug with ingredients \n",
    "    if len(ingredients)>context_length:\n",
    "        ingredients = ingredients[:context_length]\n",
    "\n",
    "    ingr_text.append(ingredients)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "    liste_ingredients_tensor[i] = clip.tokenize([ingredients]).to(device)\n",
    "    \n",
    "\n",
    "#torch.save(liste_ingredients_tensor, \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/All_ingredients_tensor.pt\")\n",
    " \n",
    "\n",
    "ingr=ingr_text[:5]\n",
    "label=labels[:5]\n",
    "\n",
    "print(ingr)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([0.0359, 0.0140, 0.0468, 0.8823, 0.0208], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Value: 0.8823 for index : 3\n",
      "Value: 0.04678 for index : 2\n",
      "Value: 0.03586 for index : 0\n",
      "Value: 0.02075 for index : 4\n",
      "Value: 0.014046 for index : 1\n",
      "3\n",
      "to pound round Italian loaf cut into inch cubescupstablespoons olive oil divi\n",
      "Bread can be toasted 3 days ahead and kept once cool in a sealed bag at room \n"
     ]
    }
   ],
   "source": [
    "image_to_print=[]\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "text = clip.tokenize(ingr).to(device)\n",
    "print(type(text))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "   \n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(ingr_text[max_index])\n",
    "print(array_text[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with all the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13463, 77])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 11.74 GiB of which 117.94 MiB is free. Including non-PyTorch memory, this process has 11.51 GiB memory in use. Of the allocated memory 11.17 GiB is allocated by PyTorch, and 214.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image)\n\u001b[0;32m---> 15\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliste_ingredients_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Pick the top 5 most similar labels for the image\u001b[39;00m\n\u001b[1;32m     18\u001b[0m image_features \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:312\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    310\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    311\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    314\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:198\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:185\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 185\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    186\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:157\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    156\u001b[0m     orig_type \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 157\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtype(orig_type)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 11.74 GiB of which 117.94 MiB is free. Including non-PyTorch memory, this process has 11.51 GiB memory in use. Of the allocated memory 11.17 GiB is allocated by PyTorch, and 214.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_to_print=[]\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "liste_ingredients_tensor = torch.load(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/All_ingredients_tensor.pt\").to(device)\n",
    "print(liste_ingredients_tensor.shape) #Huge tensor \n",
    "\n",
    "liste_ingredients_tensor=liste_ingredients_tensor[:1000]\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "   \n",
    "    text_features = model.encode_text(liste_ingredients_tensor)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index, \"for recipe : \",labels[index])\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(ingr_text[max_index])\n",
    "print(array_text[max_index])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_recipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
