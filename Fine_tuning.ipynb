{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/hanna.mergui/Computer-Vision/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import tqdm\n",
    "from transformers import CLIPTextConfig, CLIPTextModel\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm \n",
    "import mlflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device,\"device\")\n",
    "\n",
    "\n",
    "\n",
    "#Note : launch the bert etc with the new mapping csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Retrieval metrics for aligned data\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def compute_metrics(queries, database, metric='cosine',\n",
    "                    recall_klist=(1, 5, 10), return_raw=False):\n",
    "    \"\"\"Function to compute Median Rank and Recall@k metrics given two sets of\n",
    "       aligned embeddings.\n",
    "\n",
    "    Args:\n",
    "        queries (numpy.ndarray): A NxD dimensional array containing query\n",
    "                                 embeddings.\n",
    "        database (numpy.ndarray): A NxD dimensional array containing\n",
    "                                  database embeddings.\n",
    "        metric (str): The distance metric to use to compare embeddings.\n",
    "        recall_klist (list): A list of integers with the k-values to\n",
    "                             compute recall at.\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict): A dictionary with computed values for each metric.\n",
    "    \"\"\"\n",
    "    assert isinstance(queries, np.ndarray), \"queries must be of type numpy.ndarray\"\n",
    "    assert isinstance(database, np.ndarray), \"database must be of type numpy.ndarray\"\n",
    "    assert queries.shape == database.shape, \"queries and database must have the same shape\"\n",
    "    assert len(recall_klist) > 0, \"recall_klist cannot be empty\"\n",
    "\n",
    "    # largest k to compute recall\n",
    "    max_k = int(max(recall_klist))\n",
    "\n",
    "    assert all(i >= 1 for i in recall_klist), \"all values in recall_klist must be at least 1\"\n",
    "    assert max_k <= queries.shape[0], \"the highest element in recall_klist must be lower than database.shape[0]\"\n",
    "    if any(isinstance(i, float) for i in recall_klist):\n",
    "        warnings.warn(\"All values in recall_klist should be integers. Using int(k) for all values in recall_klist.\")\n",
    "\n",
    "    dists = pairwise_distances(queries, database, metric=metric)\n",
    "\n",
    "    # find the number of elements in the ranking that have a lower distance\n",
    "    # than the positive element (whose distance is in the diagonal\n",
    "    # of the distance matrix) wrt the query. this gives the rank for each\n",
    "    # query. (+1 for 1-based indexing)\n",
    "    positions = np.count_nonzero(dists < np.diag(dists)[:, None], axis=-1) + 1\n",
    "\n",
    "    # get the topk elements for each query (topk elements with lower dist)\n",
    "    rankings = np.argpartition(dists, range(max_k), axis=-1)[:, :max_k]\n",
    "\n",
    "    # positive positions for each query (inputs are assumed to be aligned)\n",
    "    positive_idxs = np.array(range(dists.shape[0]))\n",
    "    # matrix containing a cumulative sum of topk matches for each query\n",
    "    # if cum_matches_topk[q][k] = 1, it means that the positive for query q\n",
    "    # was already found in position <=k. if not, the value at that position\n",
    "    # will be 0.\n",
    "    cum_matches_topk = np.cumsum(rankings == positive_idxs[:, None],\n",
    "                                 axis=-1)\n",
    "\n",
    "    # pre-compute all possible recall values up to k\n",
    "    recall_values = np.mean(cum_matches_topk, axis=0)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['medr'] = np.median(positions)\n",
    "\n",
    "    for index in recall_klist:\n",
    "        metrics[f'recall_{int(index)}'] = recall_values[int(index)-1]\n",
    "\n",
    "    if return_raw:\n",
    "        return metrics, {'medr': positions, 'recall': cum_matches_topk}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_bert_path = \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Summaries/export_summary_Bert.csv\"\n",
    "summary_bert = pd.read_csv(summary_bert_path)\n",
    "\n",
    "\n",
    "summary_bert_ingredients_path=\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Summaries/export_summary_bert_with_ingredients.csv\"\n",
    "summary_bert_ingredients=pd.read_csv(summary_bert_ingredients_path)\n",
    "\n",
    "\n",
    "images_path=\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/\"\n",
    "\n",
    "data_dir = \"ComputerVision_Data\"\n",
    "images_dir = \"Food Images/Food Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a clean dataset with the same number of images and text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "number of sumary recipe  13463\n",
      "number of images  13582\n",
      "after adding some filtering we have number of images  13463\n",
      "13463\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "recipe = summary_bert[\"Summary\"].tolist()\n",
    "print(type(recipe))\n",
    "print(\"number of sumary recipe \",len(recipe))\n",
    "\n",
    "#print(recipe[:2])\n",
    "\n",
    "images = os.listdir(os.path.join(data_dir, images_dir))\n",
    "#print(type(images))\n",
    "print(\"number of images \",len(images))\n",
    "\n",
    "liste=summary_bert[\"Image_Name\"].tolist()\n",
    "liste = [item + \".jpg\" for item in liste]\n",
    "\n",
    "#print(liste[:5])\n",
    "\n",
    "\n",
    "images = [image for image in images if os.path.basename(image) in liste ]  \n",
    "print(\"after adding some filtering we have number of images \",len(images))\n",
    "\n",
    "images = [image[:-4] for image in images]\n",
    "\n",
    "\n",
    "#Create a CSV with only the images and the recipe who fit together\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_path = 'ComputerVision_Data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'\n",
    "csv_path2= 'ComputerVision_Data/Mapping.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter the DataFrame based on the \"Image_Name\" column\n",
    "df_filtered = df[df['Image_Name'].isin(images)]\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file\n",
    "df_filtered.to_csv(csv_path2, index=False)\n",
    "\n",
    "print(len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/miso-butter-roast-chicken-acorn-squash-panzanella.jpg', '/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/crispy-salt-and-pepper-potatoes-dan-kluger.jpg', '/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/thanksgiving-mac-and-cheese-erick-williams.jpg', '/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg', '/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/newtons-law-apple-bourbon-cocktail.jpg']\n",
      "['Pat chicken dry with paper towels, season all over with 2 tsp. salt, and tie legs together with kitchen twine. Let sit at room temperature 1 hour.\\nMeanwhile, halve squash and scoop out seeds. Run a vegetable peeler along ridges of squash halves to remove skin. Cut each half into ½\"-thick wedges; arrange on a rimmed baking sheet.\\nCombine sage, rosemary, and 6 Tbsp. melted butter in a large bowl; pour half of mixture over squash on baking sheet. Sprinkle squash with allspice, red pepper flakes, and ½ tsp. salt and season with black pepper; toss to coat.\\nAdd bread, apples, oil, and ¼ tsp. salt to remaining herb butter in bowl; season with black pepper and toss to combine. Set aside.\\nPlace onion and vinegar in a small bowl; season with salt and toss to coat. Let sit, tossing occasionally, until ready to serve.\\nPlace a rack in middle and lower third of oven; preheat to 425°F. Mix miso and 3 Tbsp. room-temperature butter in a small bowl until smooth. Pat chicken dry with paper towels, then rub or brush all over with miso butter. Place chicken in a large cast-iron skillet and roast on middle rack until an instant-read thermometer inserted into the thickest part of breast registers 155°F, 50–60 minutes. (Temperature will climb to 165°F while chicken rests.) Let chicken rest in skillet at least 5 minutes, then transfer to a plate; reserve skillet.\\nMeanwhile, roast squash on lower rack until mostly tender, about 25 minutes. Remove from oven and scatter reserved bread mixture over, spreading into as even a layer as you can manage. Return to oven and roast until bread is golden brown and crisp and apples are tender, about 15 minutes. Remove from oven, drain pickled onions, and toss to combine. Transfer to a serving dish.\\nUsing your fingers, mash flour and butter in a small bowl to combine.\\nSet reserved skillet with chicken drippings over medium heat. You should have about ¼ cup, but a little over or under is all good. (If you have significantly more, drain off and set excess aside.) Add wine and cook, stirring often and scraping up any browned bits with a wooden spoon, until bits are loosened and wine is reduced by about half (you should be able to smell the wine), about 2 minutes. Add butter mixture; cook, stirring often, until a smooth paste forms, about 2 minutes. Add broth and any reserved drippings and cook, stirring constantly, until combined and thickened, 6–8 minutes. Remove from heat and stir in miso. Taste and season with salt and black pepper.\\nServe chicken with gravy and squash panzanella alongside.', 'Preheat oven to 400°F and line a rimmed baking sheet with parchment. In a large bowl, whisk the egg whites until foamy (there shouldn’t be any liquid whites in the bowl). Add the potatoes and toss until they’re well coated with the egg whites, then transfer to a strainer or colander and let the excess whites drain. Season the potatoes with the salt, pepper, and herbs. Scatter the potatoes on the baking sheet (make sure they’re not touching) and roast until the potatoes are very crispy and tender when poked with a knife, 15 to 20 minutes (depending on the size of the potatoes).\\nTransfer to a bowl and serve.', 'Place a rack in middle of oven; preheat to 400°. Bring evaporated milk and whole milk to a bare simmer in a large saucepan over medium heat. Whisk in garlic powder, onion powder, paprika, pepper, and 1 tsp. salt. Working in batches, whisk in three fourths of the cheddar, then all of the cream cheese.\\nMeanwhile, bring a large pot of generously salted water to a boil (it should have a little less salt than seawater). Cook macaroni, stirring occasionally, until very al dente, about 4 minutes. Drain in a colander.\\nAdd macaroni to cheese sauce in pan and mix until well coated. Evenly spread out half of macaroni mixture in a 13x9\" baking dish. Sprinkle half of remaining cheddar evenly over. Layer remaining macaroni mixture on top and sprinkle with remaining cheddar. Bake until all of the cheese is melted, about 10 minutes. Let cool slightly before serving.', \"Preheat oven to 350°F with rack in middle. Generously butter baking dish.\\nPut bread in 2 shallow baking pans and bake, switching position of pans halfway through baking, until just dried out, about 10 minutes.\\nHeat 1 tablespoon oil in a 12-inch heavy skillet over medium-high heat until it shimmers, then cook half of sausage, stirring and breaking it into small pieces, until golden brown, about 6 minutes. Transfer with a slotted spoon to a large bowl. Brown remaining sausage in remaining tablespoon oil, transferring to bowl.\\nPour off fat from skillet and wipe clean. Heat butter over medium heat until foam subsides, then cook onions, celery, garlic, and ½ teaspoon each of salt and pepper, stirring occasionally, until golden, 12 to 15 minutes. Add vegetables and bread to sausage.\\nWhisk together eggs, ½ cup cream, turkey stock, cheese, and parsley, then stir into stuffing and cool completely, about 30 minutes. Reserve 5 cups stuffing to stuff turkey and spoon remainder into baking dish, then drizzle with remaining ¼ cup cream. Cover stuffing and chill.\\nAbout 1 hour before stuffed turkey is finished roasting, bring dish of stuffing to room temperature. When turkey is done, increase oven temperature to 425°F and bake stuffing, covered tightly with foil, until hot throughout, about 20 minutes. Remove foil and bake until top is golden and crisp, about 15 minutes more.\\nCooks' Note: Bread can be toasted 3 days ahead and kept (once cool) in a sealed bag at room temperature. Stuffing can be prepared (but not baked) 4 hours before roasting turkey. If baking stuffing at the same time as potatoes, put stuffing in upper third of oven and potatoes in bottom third (allow extra time).\", 'Stir together brown sugar and hot water in a cocktail shaker to dissolve. Let cool, then add bourbon, lemon juice, and apple butter and fill with ice. Shake until well chilled, about 15 seconds. Strain into an ice-filled rocks glass. Garnish with orange twist and cinnamon.']\n",
      "['[\\'1 (3½–4-lb.) whole chicken\\', \\'2¾ tsp. kosher salt, divided, plus more\\', \\'2 small acorn squash (about 3 lb. total)\\', \\'2 Tbsp. finely chopped sage\\', \\'1 Tbsp. finely chopped rosemary\\', \\'6 Tbsp. unsalted butter, melted, plus 3 Tbsp. room temperature\\', \\'¼ tsp. ground allspice\\', \\'Pinch of crushed red pepper flakes\\', \\'Freshly ground black pepper\\', \\'⅓ loaf good-quality sturdy white bread, torn into 1\" pieces (about 2½ cups)\\', \\'2 medium apples (such as Gala or Pink Lady; about 14 oz. total), cored, cut into 1\" pieces\\', \\'2 Tbsp. extra-virgin olive oil\\', \\'½ small red onion, thinly sliced\\', \\'3 Tbsp. apple cider vinegar\\', \\'1 Tbsp. white miso\\', \\'¼ cup all-purpose flour\\', \\'2 Tbsp. unsalted butter, room temperature\\', \\'¼ cup dry white wine\\', \\'2 cups unsalted chicken broth\\', \\'2 tsp. white miso\\', \\'Kosher salt, freshly ground pepper\\']', \"['2 large egg whites', '1 pound new potatoes (about 1 inch in diameter)', '2 teaspoons kosher salt', '¾ teaspoon finely ground black pepper', '1 teaspoon finely chopped rosemary', '1 teaspoon finely chopped thyme', '1 teaspoon finely chopped parsley']\", \"['1 cup evaporated milk', '1 cup whole milk', '1 tsp. garlic powder', '1 tsp. onion powder', '1 tsp. smoked paprika', '½ tsp. freshly ground black pepper', '1 tsp. kosher salt, plus more', '2 lb. extra-sharp cheddar, coarsely grated', '4 oz. full-fat cream cheese', '1 lb. elbow macaroni']\", \"['1 (¾- to 1-pound) round Italian loaf, cut into 1-inch cubes (8 cups)', '2 tablespoons olive oil, divided', '2 pounds sweet Italian sausage, casings removed, divided', '1 stick unsalted butter, cut into pieces', '3 medium onions, chopped', '4 large celery ribs, chopped', '5 garlic cloves, minced', '4 large eggs, lightly beaten', '¾ cup heavy cream, divided', '½ cup turkey giblet stock or reduced-sodium chicken broth', '1 cup grated Parmigiano-Reggiano (2 ounces)', '½ cup coarsely chopped flat-leaf parsley', '4-qt shallow ceramic or glass baking dish']\", \"['1 teaspoon dark brown sugar', '1 teaspoon hot water', '1 ½ oz. bourbon', '½ oz. fresh lemon juice', '2 teaspoons apple butter (storebought or homemade)', 'Garnish: orange twist and freshly grated or ground cinnamon']\"]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Mapping.csv\"\n",
    "dataset = pd.read_csv(dataset)\n",
    "\n",
    "Instructions=dataset[\"Instructions\"].tolist()\n",
    "Ingredients=dataset[\"Ingredients\"].tolist()\n",
    "images=dataset[\"Image_Name\"]\n",
    "\n",
    "path_folder_image= \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/\"\n",
    "images = [os.path.join(path_folder_image, image) for image in images]\n",
    "images = [item + \".jpg\" for item in images]\n",
    "\n",
    "print(images[:5])\n",
    "print(Instructions[:5])\n",
    "print(Ingredients[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data processing on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whole chicken kosher salt divided plus moresmall acorn squash about total fin', ' large egg whitespound new potatoes aboutinch in diameterteaspoons kosher sal', ' cup evaporated milkcup whole milk garlic powder onion powder smoked paprika ', 'to pound round Italian loaf cut into inch cubescupstablespoons olive oil divi', ' teaspoon dark brown sugarteaspoon hot water oz bourbonoz fresh lemon juicete']\n"
     ]
    }
   ],
   "source": [
    "to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "\n",
    "for i in range(len(Ingredients)): \n",
    "    temp = Ingredients[i].translate(str.maketrans(\"\", \"\",to_supress))\n",
    "    temp= ''.join([char for char in temp if not char.isdigit()])\n",
    "    Ingredients[i]=temp.replace(\"Tbsp\", \"\").replace(\"½\", \"\").replace(\"¾\", \"\").replace(\"lb\", \"\").replace(\"tsp\", \"\").replace(\"⅓\", \"\").replace(\"  \", \"\").replace(\"¼\", \"\")\n",
    "    \n",
    "    #A remplacer quand on aura trouvé la bonne manière de rallonger le context length\n",
    "    if len(Ingredients)>77:\n",
    "        Ingredients[i]=Ingredients[i][:77]\n",
    "\n",
    "print(Ingredients[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image,title\n",
    "    \n",
    "list_image_path = images \n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/users/eleves-b/2022/hanna.mergui/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Attempt to open cnn_infer failed: handle=0 error: libcudnn_cnn_infer.so.8: cannot open shared object file: No such file or directory (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:78.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 2/2 [05:22<00:00, 161.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Latest Update : 18 July 2022, 09:55 GMT+7\n",
    "\n",
    "# TO ADD :\n",
    "# Gradient Checkpointing\n",
    "# Filter out bias from weight decay\n",
    "# Decaying learning rate with cosine schedule\n",
    "# Half-precision Adam statistics\n",
    "# Half-precision stochastically rounded text encoder weights were used\n",
    "\n",
    "#BATCH_SIZE must larger than 1\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "BATCH_SIZE=8\n",
    "EPOCH=2\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# use your own data\n",
    "list_image_path = images \n",
    "list_txt = Ingredients\n",
    "dataset = image_title_dataset(list_image_path,list_txt)\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "  for batch in train_dataloader :\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      images,texts = batch \n",
    "      #print(\"image shape\", images.shape) #[8, 3, 224, 224]\n",
    "      #print(\"text shape\",texts.shape) #[8, 77]\n",
    "    \n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "    \n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "\n",
    "\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Checkpoints/checkpoint_number_1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of mlflow to track the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, jit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m dataset \u001b[38;5;241m=\u001b[39m image_title_dataset(list_image_path, Ingredients)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run()\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, preprocess, dataset, device, EPOCH, BATCH_SIZE, checkpoint_dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, preprocess, dataset, device, EPOCH, BATCH_SIZE, checkpoint_dir):\n\u001b[1;32m      5\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[0;32m----> 6\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(EPOCH)):\n\u001b[1;32m      9\u001b[0m         query_embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train(model, preprocess, dataset, device, EPOCH, BATCH_SIZE, checkpoint_dir):\n",
    "    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in tqdm(range(EPOCH)):\n",
    "        query_embeddings = []\n",
    "        database_embeddings = []\n",
    "        \n",
    "        for images, texts in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_embeddings = model.encode_image(images)\n",
    "                text_embeddings = model.encode_text(texts)\n",
    "            \n",
    "            query_embeddings.append(text_embeddings.cpu().numpy())\n",
    "            database_embeddings.append(image_embeddings.cpu().numpy())\n",
    "        \n",
    "        query_embeddings = np.concatenate(query_embeddings, axis=0)\n",
    "        database_embeddings = np.concatenate(database_embeddings, axis=0)\n",
    "        \n",
    "        metrics = compute_metrics(query_embeddings, database_embeddings, recall_klist=(1, 5, 10))\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value, step=epoch)\n",
    "        \n",
    "        # Optionnel : enregistrement du modèle à des époques spécifiques\n",
    "        if epoch == EPOCH - 1:  # Enregistre tous les 10 époques et à la dernière époque\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assurez-vous de définir 'device', 'list_image_path', et 'Ingredients' avant d'appeler train\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Définissez le chemin du dossier où vous souhaitez sauvegarder les checkpoints\n",
    "    checkpoint_dir = \"ComputerVision_Data/Checkpoints/\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    mlflow.start_run(run_name=\"Training with Recall Metrics\")\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCH = 10\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    dataset = image_title_dataset(list_image_path, Ingredients)\n",
    "    \n",
    "    train(model, preprocess, dataset, device, EPOCH, BATCH_SIZE, checkpoint_dir)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: EPOCH,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: total_loss,\n\u001b[1;32m      6\u001b[0m         }, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Checkpoints/checkpoint_number_3\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "        'epoch': EPOCH,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Checkpoints/checkpoint_number_3\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
