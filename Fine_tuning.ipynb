{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/ComputerVision_Project/env_recipe/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_bert_path = \"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Summaries/export_summary_Bert.csv\"\n",
    "summary_bert = pd.read_csv(summary_bert_path)\n",
    "\n",
    "data_dir = \"ComputerVision_Data\"\n",
    "images_dir = \"Food Images/Food Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sumary recipe  13463\n",
      "number of images  3486\n"
     ]
    }
   ],
   "source": [
    "recipe = summary_bert[\"Summary\"]\n",
    "print(\"number of sumary recipe \",len(recipe))\n",
    "\n",
    "images = os.listdir(os.path.join(data_dir, images_dir))\n",
    "print(\"number of images \",len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 402M/402M [02:36<00:00, 2.70MiB/s]\n",
      "/Data/ComputerVision_Project/env_recipe/lib64/python3.9/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA RTX A2000 12GB with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.\n",
      "If you want to use the NVIDIA RTX A2000 12GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "model_name = 'RN50x4'\n",
    "model, preprocess = clip.load(model_name)\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roast chicken in a large cast-iron skillet until an instant-read thermometer inserted into the thickest part of breast registers 155°F, 50–60 minutes. Meanwhile, roast squash on lower rack until mostly tender, about 25 minutes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input Place 2 chamomile tea bags in a heatsafe vessel, such as a large liquid measuring cup. Pour in 1 ½ cups boiling water, and let steep 5 minutes, then remove tea bags.\nAdd 1 ½ oz. reposado tequila, ¾ oz. fresh lemon juice, and 1 Tbsp. agave nectar and stir until incorporated. Pour into a 16-ounce insulated mug (or two smaller 8-ounce mugs) and drink hot. is too long for context length 77",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         var\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m         summary \u001b[38;5;241m=\u001b[39m summary[:context_length]  \u001b[38;5;66;03m# Truncate the text if it's too long\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     text_tensor[i] \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Access row data using integer indices\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage_Name\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Access row data using integer indices\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(var)\n",
      "File \u001b[0;32m/Data/ComputerVision_Project/env_recipe/lib64/python3.9/site-packages/clip/clip.py:234\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    232\u001b[0m             tokens[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m eot_token\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtexts[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is too long for context length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m     result[i, :\u001b[38;5;28mlen\u001b[39m(tokens)] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokens)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input Place 2 chamomile tea bags in a heatsafe vessel, such as a large liquid measuring cup. Pour in 1 ½ cups boiling water, and let steep 5 minutes, then remove tea bags.\nAdd 1 ½ oz. reposado tequila, ¾ oz. fresh lemon juice, and 1 Tbsp. agave nectar and stir until incorporated. Pour into a 16-ounce insulated mug (or two smaller 8-ounce mugs) and drink hot. is too long for context length 77"
     ]
    }
   ],
   "source": [
    "var=0\n",
    "\n",
    "# Creation of a tensor for the text \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "text_tensor = torch.zeros(len(summary_bert), context_length, dtype=torch.long)\n",
    "labels = []\n",
    "for i, row in enumerate(summary_bert.iterrows()):\n",
    "    summary = row[1][\"Summary\"]\n",
    "    if i==0: \n",
    "        print(summary)\n",
    "    if len(summary.split()) > context_length:\n",
    "        var+=1\n",
    "        summary = summary[:context_length]  # Truncate the text if it's too long\n",
    "    text_tensor[i] = clip.tokenize([summary]).to(device)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "print(var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Input Place 2 chamomile tea bags in a heatsafe vessel, such as a large liquid measuring cup. Pour in 1 ½ cups boiling water, and let steep 5 minutes, then remove tea bags. Add 1 ½ oz. reposado tequila, ¾ oz. fresh lemon juice, and 1 Tbsp. agave nectar and stir until incorporated. Pour into a 16-ounce insulated mug (or two smaller 8-ounce mugs) and drink hot.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   512, 29535,  1445,   273,  1290,   547, 18398,   600,   596,\n",
       "           602,  1303,    64, 12175,  2901, 16387,   267,   605, 10717, 34333,\n",
       "            70,  1018,   666,  1568,  5867,   565,   653,  1937,   269, 33112,\n",
       "           530,   272, 33613,   729,  3982,   647,   543,   653,  1573,   267,\n",
       "           672,  1898, 20714,   276,  3056,   267,  1594, 49407,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = test.split()[:30]\n",
    "new = \"\".join(new)\n",
    "clip.tokenize([new]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_tensor, \"text_tensor.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_recipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
